## RAG_Evaluation-RAGAs
<h1 align="center">
  <img style="vertical-align:middle" height="200"
  src="https://raw.githubusercontent.com/deepset-ai/haystack-integrations/main/logos/ragas.png">
</h1>
<p align="center">
  <i>Supercharge Your LLM Application Evaluations ðŸš€</i>
</p>



- RAGAs 

```bash
https://docs.ragas.io/en/latest/
```

Objective metrics, intelligent test generation, and data-driven insights for LLM apps

Ragas is your ultimate toolkit for evaluating and optimizing Large Language Model (LLM) applications. Say goodbye to time-consuming, subjective assessments and hello to data-driven, efficient evaluation workflows.
Don't have a test dataset ready? We also do production-aligned test set generation.

## Key Features

- ðŸŽ¯ Objective Metrics: Evaluate your LLM applications with precision using both LLM-based and traditional metrics.
- ðŸ§ª Test Data Generation: Automatically create comprehensive test datasets covering a wide range of scenarios.
- ðŸ”— Seamless Integrations: Works flawlessly with popular LLM frameworks like LangChain and major observability tools.
- ðŸ“Š Build feedback loops: Leverage production data to continually improve your LLM applications.

## :shield: Installation

Pypi: 

```bash
pip install ragas
```

Alternatively, from source:

```bash
pip install git+https://github.com/explodinggradients/ragas
```
